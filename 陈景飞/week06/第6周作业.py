import torch
import math
import numpy as np
from transformers import BertModel

'''

è®¡ç®—BERT-Baseé‡Œé¢æœ‰å¤šå°‘ä¸ªå¯è®­ç»ƒçš„å‚

'''


# æ–¹æ³•ä¸€ï¼šç›´æ¥çœ‹bert.state_dict()
def calcu_all_parameter_cnt1():
    bert = BertModel.from_pretrained(r"F:\Desktop\work_space\pretrain_models\bert-base-chinese", return_dict=False)
    state_dict = bert.state_dict()
    # print(state_dict['embeddings.word_embeddings.weight'].shape) #torch.Size([21128, 768])
    # print(state_dict['embeddings.position_embeddings.weight'].shape) #torch.Size([512, 768])
    # print(state_dict['embeddings.token_type_embeddings.weight'].shape) #torch.Size([2, 768])
    # print(state_dict['pooler.dense.weight'].shape)  # torch.Size([768, 768]) #æœ€åä¸€ä¸ªçº¿æ€§å±‚
    # print(state_dict['pooler.dense.bias'].shape)  # torch.Size([768]) #æœ€åä¸€ä¸ªçº¿æ€§å±‚

    # print(bert.state_dict().keys())  #æŸ¥çœ‹æ‰€æœ‰çš„æƒå€¼çŸ©é˜µåç§°
    # 'embeddings.word_embeddings.weight',
    # 'embeddings.position_embeddings.weight',
    # 'embeddings.token_type_embeddings.weight',
    # 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias',
    # 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias',
    # 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias',
    # 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias',
    # 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias',
    # 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias',
    # 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias',
    # 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias',
    # 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias',
    # 'pooler.dense.weight', 'pooler.dense.bias'

    # è®¡ç®—embeddingsçš„å‚æ•°é‡
    embeddings_word_embeddings_weight_para_cnt = state_dict['embeddings.word_embeddings.weight'].numel()  # (V, 768) V= 21128,
    embeddings_position_embeddings_weight_para_cnt = state_dict['embeddings.position_embeddings.weight'].numel()  # (512, 768)
    embeddings_token_type_embeddings_weight_para_cnt = state_dict['embeddings.token_type_embeddings.weight'].numel()  # (2, 768)
    embeddings_LayerNorm_weight_para_cnt = state_dict['embeddings.LayerNorm.weight'].numel()  # LayerNormå±‚ gamma (1, 768)
    embeddings_LayerNorm_bias_para_cnt = state_dict['embeddings.LayerNorm.bias'].numel()  # LayerNormå±‚ beta (1, 768)
    embeddings_para_cnt = embeddings_word_embeddings_weight_para_cnt + \
                          embeddings_position_embeddings_weight_para_cnt + \
                          embeddings_token_type_embeddings_weight_para_cnt + \
                          embeddings_LayerNorm_weight_para_cnt + \
                          embeddings_LayerNorm_bias_para_cnt



    # embeddings å’Œ lastLiner çš„keyé›†åˆ
    embeddings_and_lastLiner_keys_list = ['embeddings.word_embeddings.weight',
                                          'embeddings.position_embeddings.weight',
                                          'embeddings.token_type_embeddings.weight',
                                          'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias',
                                          'pooler.dense.weight', 'pooler.dense.bias']

    # è®¡ç®—transformersçš„å‚æ•°é‡
    transformers_para_cnt = 0
    for key, value in state_dict.items():
        if key not in embeddings_and_lastLiner_keys_list:
            transformers_para_cnt += value.numel()

    # æœ€åä¸€ä¸ªçº¿æ€§å±‚å‚æ•°
    pooler_dense_weight_para_cnt = state_dict['pooler.dense.weight'].numel()  # W (768, 768)
    pooler_dense_bias_para_cnt = state_dict['pooler.dense.bias'].numel()  # b (1, 768)
    lastLiner_para_cnt = pooler_dense_weight_para_cnt + pooler_dense_bias_para_cnt

    # æ±‡æ€»
    bert_base_all_para_cnt = embeddings_para_cnt + lastLiner_para_cnt + transformers_para_cnt * 12
    print("æ–¹æ³•1ç»“æœ= ", bert_base_all_para_cnt)


# æ–¹æ³•äºŒï¼šæŒ‰æ­¥éª¤è®¡ç®—
def calcu_all_parameter_cnt2(vocab_size, hidden_size, num_layers):
    # è®¡ç®—embeddingsçš„å‚æ•°é‡
    token_embedding_para_cnt = vocab_size * hidden_size  # token embedding (V, 768)
    segment_embedding_para_cnt = 2 * hidden_size  # segment embedding (2, 768)
    position_embedding_para_cnt = 512 * hidden_size  # position embedding (512, 768)
    embeddings_LayerNorm_weight_para_cnt = hidden_size * hidden_size  # embeddingsåçš„LayerNormå±‚ gamma (1, 768)
    embeddings_LayerNorm_bias_para_cnt = 1 * hidden_size  # embeddingsåçš„LayerNormå±‚ beta (1, 768)
    embeddings_para_cnt = token_embedding_para_cnt + \
                          segment_embedding_para_cnt + \
                          position_embedding_para_cnt + \
                          embeddings_LayerNorm_weight_para_cnt + \
                          embeddings_LayerNorm_bias_para_cnt

    # è®¡ç®—multi-headså‚æ•°é‡ï¼ˆ12 * 64 = 768 ï¼Œ12å¤´ï¼‰
    # X(L,768)
    # Q_w(768,64) Q_b(1,64) --->Q(L,64)
    # K_w(768,64) K_b(1,64) --->K(L,64)
    # V_w(768,64) V_b(1,64) --->V(L,64)
    # Q*KT --->(L,L)
    # âˆšdk=âˆš64=8
    # Q * KT * V ---> ï¼ˆL,64ï¼‰

    # è®¡ç®—Q K V ä¸‰ä¸ªçº¿æ€§å±‚çš„å‚æ•°
    encoder_layer_attention_self_query_weight_para_cnt = hidden_size * 64  # Q_w(768,64)
    encoder_layer_attention_self_query_bias_para_cnt = 1 * 64  # K_b(1,64)
    encoder_layer_attention_self_key_weight_para_cnt = hidden_size * 64  # K_w(768,64)
    encoder_layer_attention_self_key_bias_para_cnt = 1 * 64  # Q_b(1,64)
    encoder_layer_attention_self_value_weight_para_cnt = hidden_size * 64  # V_w(768,64)
    encoder_layer_attention_self_value_bias_para_cnt = 1 * 64  # V_b(1,64)

    # ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ = ğ¿ğ‘–ğ‘›ğ‘’ğ‘Ÿ(ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„, ğ¾, ğ‘‰))
    encoder_layer_attention_output_dense_weight_para_cnt = hidden_size * hidden_size  # ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› åçš„çº¿æ€§å±‚W
    encoder_layer_attention_output_dense_bias_para_cnt = 1 * hidden_size  # ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› åçš„çº¿æ€§å±‚b

    # ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› åçš„LayerNormå±‚ LayerNorm(Xembedding+ Xattention)
    encoder_layer_attention_output_LayerNorm_weight_para_cnt = 1 * hidden_size  # LayerNormå±‚ gamma (1, 768)
    encoder_layer_attention_output_LayerNorm_bias_para_cnt = 1 * hidden_size  # LayerNormå±‚ beta (1, 768)

    attention_para_cnt = (encoder_layer_attention_self_query_weight_para_cnt + \
                          encoder_layer_attention_self_query_bias_para_cnt + \
                          encoder_layer_attention_self_key_weight_para_cnt + \
                          encoder_layer_attention_self_key_bias_para_cnt + \
                          encoder_layer_attention_self_value_weight_para_cnt + \
                          encoder_layer_attention_self_value_bias_para_cnt) * 12 + \
                         encoder_layer_attention_output_dense_weight_para_cnt + \
                         encoder_layer_attention_output_dense_bias_para_cnt + \
                         encoder_layer_attention_output_LayerNorm_weight_para_cnt + \
                         encoder_layer_attention_output_LayerNorm_bias_para_cnt

    # è®¡ç®—FeedForwardå±‚çš„å‚æ•°
    # ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ = ğ¿ğ‘–ğ‘›ğ‘’ğ‘Ÿ(ğ‘”ğ‘’ğ‘™ğ‘¢(ğ¿ğ‘–ğ‘›ğ‘’ğ‘Ÿ(ğ‘¥)))
    # Bertæ²¿ç”¨äº†æƒ¯ç”¨çš„å…¨è¿æ¥å±‚å¤§å°è®¾ç½®ï¼Œå³4 * dmodle = 3072ï¼Œå› æ­¤ï¼ŒW1ï¼ŒW2åˆ†åˆ«ä¸ºï¼ˆ768, 3072)ï¼Œï¼ˆ3072, 768ï¼‰
    encoder_layer_intermediate_dense_weight_para_cnt = 768 * 3702  # é‡Œé¢çš„çº¿æ€§å±‚W ï¼ˆ768, 3072ï¼‰
    encoder_layer_intermediate_dense_bias_para_cnt = 1 * 3702  # é‡Œé¢çš„çº¿æ€§å±‚b ï¼ˆ1, 3702ï¼‰
    encoder_layer_output_dense_weight_para_cnt = 3072 * 768  # å¤–é¢çš„çº¿æ€§å±‚W ï¼ˆ3072, 768ï¼‰
    encoder_layer_output_dense_bias_para_cnt = 1 * 768  # å¤–é¢çš„çº¿æ€§å±‚b ï¼ˆ1, 3702ï¼‰

    # FeedForwardå±‚åçš„LayerNormå±‚
    encoder_layer_output_LayerNorm_weight = 1 * hidden_size  # LayerNormå±‚ gamma (1, 768)
    encoder_layer_output_LayerNorm_bias = 1 * hidden_size  # LayerNormå±‚ beta (1, 768)

    feed_forward_para_cnt = encoder_layer_intermediate_dense_weight_para_cnt + \
                            encoder_layer_intermediate_dense_bias_para_cnt + \
                            encoder_layer_output_dense_weight_para_cnt + \
                            encoder_layer_output_dense_bias_para_cnt + \
                            encoder_layer_output_LayerNorm_weight + \
                            encoder_layer_output_LayerNorm_bias

    # æœ€åä¸€ä¸ªçº¿æ€§å±‚å‚æ•°
    pooler_dense_weight_para_cnt = hidden_size * hidden_size  # W (768, 768)
    pooler_dense_bias_para_cnt = 1 * hidden_size  # b (1, 768)
    lastLiner_para_cnt = pooler_dense_weight_para_cnt + pooler_dense_bias_para_cnt

    # æ±‡æ€»
    bert_base_all_para_cnt = embeddings_para_cnt + (attention_para_cnt + feed_forward_para_cnt) * num_layers + lastLiner_para_cnt
    print("æ–¹æ³•2ç»“æœ= ", bert_base_all_para_cnt)


if __name__ == "__main__":
    calcu_all_parameter_cnt1()  # 102267648 â‰ˆ 102 M â‰ˆ 0.1B
    calcu_all_parameter_cnt2(21128, 768, 12)  # 108670344  â‰ˆ 108 M â‰ˆ 0.1B
